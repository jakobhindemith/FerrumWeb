# ğŸ•¸ï¸ webcrawler_links

A recursive **Rust** web crawler that visits websites, extracts links, and stores them in a **SQLite database**. Each link is saved with its **parent URL** and **depth level**, allowing a structured hierarchy of web connections.

## ğŸ” Features

- ğŸŒ Recursively crawls websites up to a configurable depth  
- ğŸ”— Extracts all HTML links (`<a href="...">`)  
- ğŸ§­ Stores:
  - the discovered URL,
  - its parent URL,
  - the crawl depth level,
  - and a unique ID
- ğŸ—ƒï¸ Persists data in a SQLite database (table: `link`)
- ğŸ§¹ Filters duplicate links (optional aggregation)
- â³ Optional delay between requests to avoid rate-limiting

## ğŸ› ï¸ Technologies

- [Rust](https://www.rust-lang.org/)
- [`reqwest`](https://docs.rs/reqwest/) â€“ HTTP client  
- [`scraper`](https://docs.rs/scraper/) â€“ HTML parser  
- [`rusqlite`](https://docs.rs/rusqlite/) â€“ SQLite database integration

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/webcrawler_links.git
cd webcrawler_links
cargo run
```

## ğŸ’¡Usage

When the program starts, it prompts for a URL and begins crawling from that page. All discovered links are stored recursively in a database. The resulting structure is useful for analyzing site architectures, detecting broken links, or conducting SEO audits.
ğŸ§ª Example SQL Queries

  Count of distinct URLs:
  ```sql
  SELECT COUNT(DISTINCT URL) FROM link;
  ```
  Grouped links by frequency:
  ```sql
    SELECT URL, COUNT(*) AS count FROM link GROUP BY URL ORDER BY count DESC;
  ```

## âš ï¸ Disclaimer

This tool is intended for educational and research purposes only. Please respect website robots.txt policies and use responsibly.
